## Result Replication

### HinDroid Intuition

The HinDroid model calculates similarities between apps and feeds the similarities to a SVM to form a decision boundary between two data classes. It constructs a Heterogeneous Information Network (HIN) to capture the relationships between apps and between APIs. The network consists of two node types and four edge types. Edge type A connects apps to APIs if the API is used by that app. Edge type B connects every pair of APIs that are co-appeared inside the same code block in every app. Edge type P connects every pair of APIs that are from the same library (package). Edge type I connects every pair of APIs that are using the same invoke method for present in every app. Each type of these edges will be represented by an adjacency matrix. To calculate the similarity, we formalize it as the number of common features that are both present in the HIN. If the similarity between two apps is high, there may be something to be said for it. We define this common feature as the number of metapaths between apps. A metapath starts from an app and ends with an app and it goes through a symmetric node path in the HIN. For example, A-A means how many APIs are common within a pair of apps; A-P-A means how many pairs of APIs (one API from a<sub>i</sub>, one API from a<sub>j</sub>) that use a common library. HinDroid assumes these metapaths represent a unique common feature between applications. For the case of metapath A-B-A, if two apps both called these specific APIs within a block somewhere in the source code, then their functionality or intent should be similar. Based on this intuition, we can see that if an app is more similar to a malware in terms of high similarity, we can be more confident that the mystery app should be a malware as well. In implementation, these metapaths are calculated using multiplication of adjacency matrices, and the result is used as a Gram matrix for the SVM algorithm to find a decision boundary between the two classes. Next we are going to talk about how to come to that result.

### Replication Methodology and Process

From the pre-processing pipeline in EDA, we extracted the features needed for the HinDroid model as a csv format for each app. This step then follow through after the csvs are saved to file and are read again.

Using the csv from the previous step, we have the necessary information to construct the edge type A, B, P, I in the Heterogeneous Information Network. Matrix A will have a shape of (# of apps, # of APIs) and matrices {B,P,I} will be symmetric and have a shape of (# of APIs, # of APIs). As the number of apps increases, the # of APIs present in the codebase grows as well. Matrices {A,B,P} will be saved as Compressed Sparse Row matrices ([`scipy.sparse.csr_matrix`](https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse)) and matrix I will not be considered in this replication because of its high density.

To generate the A adjacency matrix, we used a unique ID finder (src.utils.UniqueIdAssigner) that assigns an unique integer ID to each unique API. We then keep a set of API IDs that appear in an app for each app and save these sets as a sparse matrix. The table of the one-to-one association between API and ID is also saved.

For adjacency matrices B and P, we do the following for each edge B and P. First, we generate pairs of API IDs from `itertools.combinations(APIs, 2)` for each condition that follows the edge type. This result in a matrix of shape (2, # of pairs) where every column of this matrix is a pair of API which are connected in the HIN, so that its corresponding value () in the final matrix should be 1. This step is parallelized by using multiprocessing. Then, these matrices are horizontally stacked together to create a sparse matrix in COOrdinate format ([`scipy.sparse.coo_matrix`](https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse)). The pair of IDs are extracted because conversion between COO format and CSR format are "efficient, linear-time operations", and CSR format are suitable for matrix multiplication which is essential for this model. As far as I know, this should be the most efficient way of constructing the adjacency matrices, both using parallelism and using appropriate aggregation methods.

The number of meta-paths between the apps can then be calculated by multiplying the A matrix to other adjacency matrices and their appropriate inverses. For example, metapath `APBP^TA^T` will amount to `A.dot(P).dot(B).dot(P.T).dot(A.T)` in matrix multiplication.

### Replication Details

For each app, there is a csv file where each row is an API call in the codebase and its relevant details are also stored according to the schema in EDA. These csv files are stored inside the `process` directory under `data`, ungrouped. The aggregation happens afterwords and the coordinates are saved for each app in a separate numpy file. The naming scheme follows `{app_package}.B.npy` or `{app_package}.P.npy`. These matrices are of shape (2, # number of pairs). The aggregated A, B, P matrices are first stored as COO format in memory and written to disk in CSR format. These files are saved under `processed` using the `A.npz`/`B.npz`/`P.npz` naming scheme.

As the number of apps in the dataset grows, the number of API goes up as well. For 10 apps, there will be around 80k APIs; and for 1000 apps, there will be around 1.5M. The growth of the unique APIs is sublinear to the number of apps, and intuitively, as the the dimension increases, the number of values in the sparse matrices should increases quadratically. However, since CSR format only stores the non-zero values along a row, we can say that the file size increases linearly. We also keep a record of the mapping between API and their ids in a csv file. For around 1000 apps, the A sparse matrix file (`A.npz`) takes 15MB; and for B (`B.npz`) and P (`P.npz`), it's 47MB and 11MB respectively. The list of unique APIs (`APIs.csv`) takes up 88MB of space. These file sizes are within reasonable bounds and much much smaller than the raw data.

### Experiment Results and Interpretation

The dataset consists of 500 benign apps and 500 malware. In the benign app, we sampled 100 apps from each of the categories Communication, Tools, Comics, Art and Design, and Beauty. We assume these apps will have an adequate codebase size (as opposed to that in gaming categories) and should represent a wide ranging of APIs in the sample space. After pruning large or invalid smali codebases, there remain 481 benign apps and 484 malware. The dataset is then splitted into training and testing sets using a 2:1 ratio. We evaluate the performance of four metapaths in the paper: `AA^T`, `ABA^T`, `APA^T`, `APBP^TA^T`. Using 11 trials of random splitting, we have the following metrics. Only the median of each metric is shown.

| metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|----------|-----------|----------|--------|-------|------|-------|------|
| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |
| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |
| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |
| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |

The training accuracies of `AA` and `APA` are always 1.0. Interestingly, `AA` performs the best across both testing accuracies and F1 scores. Intuitively, adding more information should increase the testing accuracy, but here, the accuracy goes down when the metapath gets more complex. The F1 scores are very close to the corresponding accuracy because the testing set has balanced labels thanks to the curated dataset. In dealing with hardware, false negatives should be the most dangerous as they can slip through the detection and do actual harm. We see that in our implementation, FNs are smaller for the simpler metapaths. Adding more information in the kernel obfuscated the similarity and make the decision boundary less confident. The reason for this is explained in the next section.

To compare to the results referenced in the HinDroid paper, we should use F1 scores because the testing set in HinDroid's implementation has an imbalance of true and false labels. The F1 scores for `AA` are virtually the same. The scores for `APA` did slightly worse than HinDroid's with a 2% difference. For `ABA` and `APBPA`, the F1 scores are much worse than HinDroid's, getting at least a 10% drop. It seems like the more complex metapaths did not give more relevant information to aid the classification.

### Shortcomings and Possible Improvements

There is one important weakness in my implementation of the preprocessing stage that may lead to data leakage into the model and it is also part of the reason that simpler metapath has a higher accuracy. The training and testing data as a whole are passed through the matrix generating process exactly once, so the APIs will include all the seen APIs in the training and testing dataset, instead of keeping two separate sets of matrices {A, B, P}. Only at training time, A matrix is splitted into training rows and testing rows to calculate the Gram matrix, but the feature vector representing an app has a dimension of the number of unique APIs in the whole dataset, which prompted the data leakage. Furthermore, in adjacency matrices B and P, there is no way to discern whether the nonzero values (1s) are from the connections in the training set. The train-test split should be done at feature extraction step to isolate the features in the two classes. It is a design oversight as train-test splitting were not taken into account at the beginning of implementation design. This bug could take some effort to fix because of it.

To improve the model, it is beneficial to apply ensemble learning methods since a single metapath cannot give a clear decision boundary in its limited feature space. One method we can use is to use a voting algorithm that combines the predicted confidences of several kernels and unify a single decision. Another way is to follow the combined-kernel method which uses the Laplacian Scores for feature selection. The best method in the paper uses a multi-kernel learning algorithm, and can also be used to improve the model.

## Conclusion

Through a rough but non-trivial implementation of the HinDroid paper, we learned that handling large amounts of unstructured data can be very difficult to handle and it requires tremendous effort to construct a custom data pipeline with a user-friendly interface and high versatility. Indeed, a large portion of the data science workflow is working on data and not on the model. For replication, we tested a small portion of the HinDroid paper's findings and found some similarities and inconsistencies for the F1 scores. We also find that there are many things that could be done to improve the model, where fixing data leakage is the most significant one.
